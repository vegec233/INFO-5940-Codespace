Implementing the multi-agent workflow for the travel planner helped me understand how structured collaboration among specialized AI agents can produce more consistent and trustworthy results than relying on a single model. Additionally, dividing responsibilities between a Planner Agent and a Reviewer Agent highlighted the importance of well-crafted system prompts and precise role definitions. The Planner focused on creating the itinerary, while the Reviewer acted as a fact checker with internet access. This structure revealed how prompt design, through clear roles and explicit formatting, can directly shape the quality of cooperation and communication between agents. I also learned that prompt engineering is not only about instructing what an agent should generate, but about shaping how it reasons, balancing relevance, completeness, and user expectations. Through iterative refinement of instructions, I was able to better align model behavior with users' intent. Overall, this project deepened my understanding of how subtle design choices in prompting influence both the final output and the underlying reasoning that drives multi-agent collaboration.

One main challenge was that the Reviewer initially generated only a summary of changes instead of the full corrected itinerary. Because the original prompt stated “do not generate a new plan,” the agent interpreted its task too narrowly. Reworded instructions such as “The full, validated itinerary (integrating all fixes)” and “(show the complete corrected plan)” solved the issue and showed that subtle phrasing changes can significantly influence agent behavior.

A key design choice was adding a search efficiency constraint in the Reviewer’s prompt: “Keep network calls efficient (ideally ≤ 3 searches per day of itinerary). Combine related checks into one query when possible.” This encouraged the agent to bundle fact checks intelligently while preventing accidental overuse of the Tavily API. To maintain consistent formatting, I used Markdown headings and simple cues like *** to guide the model’s structure. These refinements demonstrated how careful prompt design, efficiency rules, and clear formatting patterns can help two agents collaborate smoothly and produce outputs that are both efficient and easy for users to follow.

I used ChatGPT for prompt refinement and Markdown formatting. This is because I was concerned that small mistakes, such as grammar errors in the prompt, might affect performance, and I am not very familiar with Markdown formatting.

You can start the chatbot by typing the command below in the terminal:
OPENAI_API_KEY="YOUR_ACTUAL_API" TAVILY_API_KEY="YOUR_ACTUAL_TAVILY_API" streamlit run assign_2.py